"""I generate report-ready PNG figures from my project outputs (data/, results/, src/) and save them into figures/."""
from __future__ import annotations

import argparse
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.inspection import permutation_importance
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import mean_squared_error
<<<<<<< HEAD
from sklearn.pipeline import Pipeline

# ----------------------------
# Paths
# ----------------------------
ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = ROOT / "data"
RESULTS_DIR = ROOT / "results"
FIG_DIR = ROOT / "figures"
SRC_DIR = ROOT / "src"

FIG_DIR.mkdir(exist_ok=True)

# This paragraph is for local imports without installing as a package.
# I add the project root to sys.path so `from src...` works when this script is run directly.
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

# IMPORTANT: I import the dataset builder used by the main pipeline so my figures match the run settings.
=======
from sklearn.neural_network import MLPRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# ----------------------------
# Paths + import safety
# ----------------------------
# This paragraph is for making the script work from any working directory (including the grader's setup).
# I add the project root to sys.path so imports from src/ still work even if I run this file directly.
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

# This paragraph is for keeping all outputs in predictable folders, so the report and dashboard can reuse them.
RESULTS_DIR = PROJECT_ROOT / "results"
FIG_DIR = PROJECT_ROOT / "figures"
FIG_DIR.mkdir(exist_ok=True)

# This paragraph is for coherence: I reuse the exact same pipeline code as main.py,
# so figures match the results and I avoid "two versions of the truth".
from src.features import build_master_table  # noqa: E402
>>>>>>> 41adbd1 (Update)
from src.models import make_dataset, time_train_test_split  # noqa: E402


@dataclass(frozen=True)
class RunConfig:
    tag: str
    mode: str  # "forecast" or "nowcast"
    covid_mode: str  # "strict" or "ex_post"
    include_macro: bool
    include_oil: bool
    start_year: Optional[int]
    test_ratio: float


# ----------------------------
# Helpers
# ----------------------------
def _rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    # This paragraph is for using one metric definition everywhere (models, tables, and figures).
    return float(np.sqrt(mean_squared_error(y_true, y_pred)))


<<<<<<< HEAD
def _save_fig(path: Path) -> None:
    plt.tight_layout()
    plt.savefig(path, dpi=200)
    plt.close()


def _read_metrics(tag: str) -> pd.DataFrame:
    p = RESULTS_DIR / f"model_metrics_{tag}.csv"
    if not p.exists():
        return pd.DataFrame()
    return pd.read_csv(p)


def _read_cv_scores(tag: str) -> pd.DataFrame:
    p = RESULTS_DIR / f"cv_scores_{tag}.csv"
    if not p.exists():
        return pd.DataFrame()
    return pd.read_csv(p)


def _pick_latest_metrics_file() -> Path:
    files = sorted(RESULTS_DIR.glob("model_metrics_*.csv"), key=lambda p: p.stat().st_mtime)
    if not files:
        raise FileNotFoundError("No model_metrics_*.csv found in results/. Run main.py first.")
    return files[-1]


def _best_model_from_metrics(metrics_df: pd.DataFrame) -> str:
    # This paragraph is for robustness: if I cannot read metrics, I fall back to a sensible default.
    if metrics_df is None or metrics_df.empty or "test_RMSE" not in metrics_df.columns:
        return "gradient_boosting"
    return metrics_df.sort_values("test_RMSE", ascending=True).iloc[0]["model"]
=======
def _imputer() -> SimpleImputer:
    # This paragraph is for compatibility across sklearn versions.
    # Some versions support keep_empty_features, others don't, so I fallback cleanly.
    try:
        return SimpleImputer(strategy="median", keep_empty_features=True)
    except TypeError:
        return SimpleImputer(strategy="median")
>>>>>>> 41adbd1 (Update)


def _infer_config_from_tag(tag: str, default_test_ratio: float = 0.2) -> RunConfig:
    # This paragraph is for reducing manual inputs: the run tag already encodes key choices,
    # so I infer the config to avoid generating figures with mismatched settings.
    t = tag.lower()
    mode = "nowcast" if "nowcast" in t else "forecast"
    covid_mode = "ex_post" if "ex_post" in t else "strict"
    include_macro = ("macro" in t) or ("withmacro" in t)
    include_oil = ("oil" in t)
    start_year = 1992 if "restricted" in t else None
    return RunConfig(
        tag=tag,
        mode=mode,
        covid_mode=covid_mode,
        include_macro=include_macro,
        include_oil=include_oil,
        start_year=start_year,
<<<<<<< HEAD
        test_ratio=float(default_test_ratio),
    )


def _config_from_metrics(metrics_df: pd.DataFrame, tag: str, *, default_test_ratio: float = 0.2) -> RunConfig:
    # This paragraph is for perfect reproducibility.
    # Instead of inferring flags from the tag string (which can drift),
    # I rebuild the figure config from the metrics CSV that main.py wrote.
    if metrics_df is None or metrics_df.empty:
        return _infer_config_from_tag(tag, default_test_ratio=default_test_ratio)

    row = metrics_df.iloc[0]

    def _get_bool(col: str, default: bool = False) -> bool:
        if col not in metrics_df.columns:
            return bool(default)
        v = row[col]
        if isinstance(v, (bool, np.bool_)):
            return bool(v)
        if isinstance(v, (int, float, np.integer, np.floating)):
            return bool(int(v))
        if isinstance(v, str):
            return v.strip().lower() in {"1", "true", "t", "yes", "y"}
        return bool(default)

    def _get_str(col: str, default: str) -> str:
        if col not in metrics_df.columns:
            return default
        v = row[col]
        return default if pd.isna(v) else str(v)

    def _get_float(col: str, default: float) -> float:
        if col not in metrics_df.columns:
            return float(default)
        v = row[col]
        try:
            return float(v)
        except Exception:
            return float(default)

    def _get_int(col: str, default: Optional[int]) -> Optional[int]:
        if col not in metrics_df.columns:
            return default
        v = row[col]
        if pd.isna(v):
            return default
        try:
            return int(v)
        except Exception:
            return default

    mode = _get_str("mode", "forecast")
    include_covid = _get_bool("include_covid", default=False)
    include_macro = _get_bool("include_macro", default=False)
    include_oil = _get_bool("include_oil", default=False)
    start_year = _get_int("start_year", default=None)
    test_ratio = _get_float("test_ratio", default_test_ratio)

    covid_mode = "ex_post" if include_covid else "strict"

    return RunConfig(
        tag=tag,
        mode=mode,
        covid_mode=covid_mode,
        include_macro=include_macro,
        include_oil=include_oil,
        start_year=start_year,
        test_ratio=test_ratio,
    )


# ----------------------------
# Model utilities (lightweight, just for figures)
# ----------------------------
def _make_model(name: str) -> object:
    if name == "linear_regression":
        return LinearRegression()
    if name == "ridge":
        return Ridge(alpha=1.0, random_state=42)
    if name == "random_forest":
        return RandomForestRegressor(n_estimators=500, random_state=42)
    # default: gradient boosting
    return HistGradientBoostingRegressor(random_state=42)


def build_predictions_for_tag(
    tag: str, *, preferred_model: Optional[str] = None
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray, np.ndarray, Pipeline, RunConfig, str]:
    """
    I rebuild the dataset exactly like main.py/run_all_models (macro/oil/covid + optional start_year),
    then refit the chosen model to regenerate prediction and diagnostic figures.
    """
    metrics = _read_metrics(tag)
    cfg = _config_from_metrics(metrics, tag)

    # These flags are the exact knobs used by src/models.make_dataset().
    include_covid = bool(cfg.covid_mode == "ex_post")
    include_macro = bool(cfg.include_macro)
    include_oil = bool(cfg.include_oil)
    start_year = cfg.start_year

    df_model, X, y = make_dataset(
        include_oil=include_oil,
        include_macro=include_macro,
        include_covid=include_covid,
        start_year=start_year,
        mode=cfg.mode,
    )

    X_train, X_test, y_train, y_test = time_train_test_split(df_model, X, y, test_ratio=cfg.test_ratio)
    split = int(len(y_train))
    years_test = df_model["year"].iloc[split:].to_numpy()

    used_model = preferred_model or _best_model_from_metrics(metrics)
    model = _make_model(used_model)

    pipe = Pipeline([("imputer", SimpleImputer(strategy="median")), ("model", model)])
    pipe.fit(X_train, y_train)

    pred = pipe.predict(X_test)

    pred_df = pd.DataFrame({"year": years_test, "actual": y_test, "pred": pred})
    pred_df["error"] = pred_df["actual"] - pred_df["pred"]
    pred_df["abs_error"] = pred_df["error"].abs()
    pred_df.to_csv(FIG_DIR / f"predictions_{tag}_{used_model}.csv", index=False)

    return df_model, X_train, X_test, y_train, y_test, years_test, pipe, cfg, used_model


# ----------------------------
# Plots
# ----------------------------
def plot_model_comparison(metrics_df: pd.DataFrame, tag: str) -> Path:
    # This paragraph is for an at-a-glance leaderboard plot for the report.
    if metrics_df is None or metrics_df.empty:
        raise FileNotFoundError(f"No metrics found for tag={tag}. Run main.py first.")

    df = metrics_df.copy()
    df = df.sort_values("test_RMSE", ascending=True)

    plt.figure()
    plt.bar(df["model"].astype(str), df["test_RMSE"].astype(float))
    plt.ylabel("Test RMSE")
    plt.title(f"Model comparison (tag={tag})")
    plt.xticks(rotation=35, ha="right")
=======
        test_ratio=default_test_ratio,
    )


def _pick_latest_metrics_file() -> Path:
    # This paragraph is for convenience: if I do not specify a tag,
    # I grab the most recent run so "python make_report_figures.py" still works.
    files = sorted(RESULTS_DIR.glob("model_metrics_*.csv"), key=lambda p: p.stat().st_mtime, reverse=True)
    if not files:
        raise FileNotFoundError("No results/model_metrics_*.csv found. Run main.py first.")
    return files[0]


def _read_metrics(tag: str) -> pd.DataFrame:
    # This paragraph is for safety: figures should fail early with a clear message
    # if results are missing, rather than silently producing empty plots.
    p = RESULTS_DIR / f"model_metrics_{tag}.csv"
    if not p.exists():
        raise FileNotFoundError(f"Missing {p}. Run main.py first.")
    return pd.read_csv(p)


def _best_model_from_metrics(metrics_df: pd.DataFrame) -> str:
    # This paragraph is for automatic selection: for the report figures,
    # I want the strongest non-baseline model according to test RMSE.
    df = metrics_df.copy()
    df = df[~df["model"].astype(str).str.startswith("baseline")]
    df = df.sort_values("test_RMSE", ascending=True)
    if df.empty:
        return "ridge"
    return str(df.iloc[0]["model"])


def _pipeline_for(model_name: str) -> Optional[Pipeline]:
    # This paragraph is for a single "factory" place where model choices live.
    # It keeps my plotting code simple and reduces the risk of using different hyperparams in different scripts.
    name = model_name.strip().lower()

    if name == "linear_regression":
        return Pipeline([("imputer", _imputer()), ("scaler", StandardScaler()), ("model", LinearRegression())])

    if name == "ridge":
        return Pipeline([("imputer", _imputer()), ("scaler", StandardScaler()), ("model", Ridge(alpha=5.0))])

    if name == "random_forest":
        return Pipeline(
            [
                ("imputer", _imputer()),
                ("model", RandomForestRegressor(n_estimators=600, max_depth=5, min_samples_leaf=2, random_state=42)),
            ]
        )

    if name == "gradient_boosting":
        return Pipeline(
            [
                ("imputer", _imputer()),
                (
                    "model",
                    HistGradientBoostingRegressor(
                        random_state=42,
                        early_stopping=False,
                        max_depth=2,
                        learning_rate=0.03,
                        max_iter=1200,
                        min_samples_leaf=20,
                        l2_regularization=0.1,
                    ),
                ),
            ]
        )

    if name == "neural_net_mlp":
        return Pipeline(
            [
                ("imputer", _imputer()),
                ("scaler", StandardScaler()),
                ("model", MLPRegressor(hidden_layer_sizes=(32, 16), alpha=1e-3, max_iter=5000, random_state=42)),
            ]
        )

    # This paragraph is for robustness: if a metrics file contains an unexpected model name,
    # I let the caller fallback to a supported model instead of crashing here.
    return None


def _save_fig(outpath: Path) -> None:
    # This paragraph is for clean report visuals: tight_layout avoids label cutoffs,
    # and closing figures prevents memory issues when I generate many plots.
    outpath.parent.mkdir(parents=True, exist_ok=True)
    plt.tight_layout()
    plt.savefig(outpath, dpi=220, bbox_inches="tight")
    plt.close()


# ----------------------------
# Plot builders
# ----------------------------
def plot_model_comparison(metrics_df: pd.DataFrame, tag: str) -> Path:
    # This paragraph is for the report: a simple bar chart makes model ranking readable fast.
    df = metrics_df.copy().sort_values("test_RMSE", ascending=True)

    plt.figure()
    plt.bar(df["model"].astype(str), df["test_RMSE"].astype(float))
    plt.xticks(rotation=45, ha="right")
    plt.ylabel("Test RMSE")
    plt.title(f"Model comparison (tag={tag})")
>>>>>>> 41adbd1 (Update)

    out = FIG_DIR / f"model_comparison_{tag}.png"
    _save_fig(out)
    return out


<<<<<<< HEAD
def plot_actual_vs_pred(pred_df: pd.DataFrame, tag: str, model_name: str) -> Path:
    # This paragraph is for showing whether the best model tracks the realized series.
    df = pred_df.sort_values("year")

    plt.figure()
    plt.plot(df["year"], df["actual"], label="Actual")
    plt.plot(df["year"], df["pred"], label=model_name)
    plt.axhline(0.0, linewidth=1.0)
=======
def build_predictions_for_tag(
    tag: str, preferred_model: Optional[str] = None
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray, np.ndarray, Pipeline, RunConfig]:
    # This paragraph is for generating "actual vs predicted" plots using the same dataset logic as main.py.
    # I rebuild the dataset instead of reusing cached arrays so this script stays standalone.
    cfg = _infer_config_from_tag(tag)
    include_covid = cfg.covid_mode == "ex_post"

    df_model, X, y = make_dataset(
        include_oil=cfg.include_oil,
        include_macro=cfg.include_macro,
        include_covid=include_covid,
        start_year=cfg.start_year,
        mode=cfg.mode,
    )

    X_train, X_test, y_train, y_test = time_train_test_split(df_model, X, y, test_ratio=cfg.test_ratio)

    # This paragraph is for alignment: I rebuild the test-year vector with the same split logic
    # so plots label the right years and match the dashboard tables.
    split_idx = int(np.floor(len(df_model) * (1 - cfg.test_ratio)))
    years_test = df_model["year"].iloc[split_idx:].astype(int).to_numpy()

    # This paragraph is for making the script usable: I either use the chosen model
    # or a sensible default, and I fallback if the name is not supported.
    model_name = preferred_model or "gradient_boosting"
    pipe = _pipeline_for(model_name)

    if pipe is None:
        for candidate in ["gradient_boosting", "random_forest", "ridge", "linear_regression", "neural_net_mlp"]:
            pipe = _pipeline_for(candidate)
            if pipe is not None:
                model_name = candidate
                break
    if pipe is None:
        raise RuntimeError("Could not instantiate any model pipeline.")

    pipe.fit(X_train, y_train)
    yhat_test = pipe.predict(X_test)

    # This paragraph is for traceability: I save a small table used in LaTeX and debugging,
    # so the figure has a data source on disk.
    pred_df = pd.DataFrame(
        {
            "year": years_test,
            "actual": y_test,
            "pred": yhat_test,
            "error": (y_test - yhat_test),
            "abs_error": np.abs(y_test - yhat_test),
        }
    )
    pred_df.to_csv(FIG_DIR / f"predictions_{tag}_{model_name}.csv", index=False)

    return df_model, X_train, X_test, y_train, y_test, years_test, pipe, cfg


def plot_actual_vs_pred(pred_df: pd.DataFrame, tag: str, model_name: str) -> Path:
    # This paragraph is for the main story in the report: do predictions track reality over the test years?
    plt.figure()
    plt.plot(pred_df["year"], pred_df["actual"], label="Actual")
    plt.plot(pred_df["year"], pred_df["pred"], label=model_name)
    plt.axhline(0.0, linewidth=1)
>>>>>>> 41adbd1 (Update)
    plt.xlabel("Year (test set)")
    plt.ylabel("GDP growth (%)")
    plt.title(f"Actual vs Predicted (tag={tag})")
    plt.legend()

    out = FIG_DIR / f"pred_vs_actual_{tag}.png"
    _save_fig(out)
    return out


def plot_abs_error_by_year(pred_df: pd.DataFrame, tag: str) -> Path:
    # This paragraph is for diagnosing unstable years (like crisis periods),
    # because averages can hide big misses on specific dates.
    df = pred_df.sort_values("year")

    plt.figure()
    plt.bar(df["year"].astype(int), df["abs_error"].astype(float))
    plt.xlabel("Year (test set)")
    plt.ylabel("|Actual - Pred|")
    plt.title(f"Absolute error by year (tag={tag})")

    out = FIG_DIR / f"error_by_year_{tag}.png"
    _save_fig(out)
    return out


<<<<<<< HEAD
def plot_errors_vs_disaster_proxy(pred_df: pd.DataFrame, df_model: pd.DataFrame, tag: str) -> Optional[Path]:
    # This paragraph is for a sanity check: if errors correlate with disaster intensity,
    # the model is missing nonlinearities or interactions tied to disasters.
    if "damage_share_gdp_lag1" not in df_model.columns:
        return None

    df = pred_df.merge(df_model[["year", "damage_share_gdp_lag1"]], on="year", how="left")
    x = pd.to_numeric(df["damage_share_gdp_lag1"], errors="coerce").to_numpy()
    y = pd.to_numeric(df["error"], errors="coerce").to_numpy()

    plt.figure()
    plt.scatter(x, y)
    plt.axhline(0.0, linewidth=1.0)
    plt.xlabel("damage_share_gdp_lag1")
=======
def plot_errors_vs_disaster_proxy(pred_df: pd.DataFrame, X_test: pd.DataFrame, tag: str) -> Optional[Path]:
    # This paragraph is for a quick sanity check: if disasters matter, errors might move with a disaster proxy.
    # I keep it simple because the sample is small and I only want a visual hint.
    if "damage_share_gdp_lag1" in X_test.columns:
        x = pd.to_numeric(X_test["damage_share_gdp_lag1"], errors="coerce").to_numpy()
        xlab = "damage_share_gdp_lag1"
    elif "n_events_lag1" in X_test.columns:
        x = pd.to_numeric(X_test["n_events_lag1"], errors="coerce").to_numpy()
        xlab = "n_events_lag1"
    else:
        return None

    plt.figure()
    plt.scatter(x, pred_df["error"].to_numpy())
    plt.axhline(0.0, linewidth=1)
    plt.xlabel(xlab)
>>>>>>> 41adbd1 (Update)
    plt.ylabel("Error (actual - pred)")
    plt.title(f"Errors vs disaster proxy (tag={tag})")

    out = FIG_DIR / f"errors_vs_disaster_{tag}.png"
    _save_fig(out)
    return out


<<<<<<< HEAD
def plot_permutation_importance(
    pipe: Pipeline,
    X_test: pd.DataFrame,
    y_test: np.ndarray,
    tag: str,
    *,
    top_k: int = 12,
) -> Path:
    # This paragraph is for interpretability: permutation importance tells which features matter for predictions.
=======
def plot_feature_importance(
    pipe: Pipeline, X_test: pd.DataFrame, y_test: np.ndarray, tag: str, top_k: int = 12
) -> Path:
    # This paragraph is for interpretability in the report.
    # Permutation importance can be noisy with small samples, but it gives a readable ranking fast.
>>>>>>> 41adbd1 (Update)
    res = permutation_importance(
        pipe,
        X_test,
        y_test,
        n_repeats=30,
        random_state=42,
        scoring="neg_root_mean_squared_error",
    )
    imp = pd.DataFrame(
        {"feature": X_test.columns, "importance": res.importances_mean, "std": res.importances_std}
    ).sort_values("importance", ascending=False)

    # This paragraph is for reproducibility: I save the table so the figure is not "just a picture".
    imp.to_csv(FIG_DIR / f"feature_importance_table_{tag}.csv", index=False)

    top = imp.head(top_k).copy()
    plt.figure()
    plt.barh(top["feature"][::-1], top["importance"][::-1])
    plt.xlabel("Permutation importance (Î”RMSE)")
    plt.title(f"Top {top_k} features (tag={tag})")

    out = FIG_DIR / f"feature_importance_{tag}.png"
    _save_fig(out)
    return out


def plot_cv_stability(tag: str) -> Optional[Path]:
    # This paragraph is for showing stability across CV folds.
    # If one model jumps a lot across folds, I do not trust its performance as much.
<<<<<<< HEAD
    df = _read_cv_scores(tag)
    if df is None or df.empty or "fold" not in df.columns or "val_RMSE" not in df.columns:
=======
    p = RESULTS_DIR / f"cv_scores_{tag}.csv"
    if not p.exists():
        return None

    df = pd.read_csv(p)
    if df.empty or "fold" not in df.columns or "val_RMSE" not in df.columns:
>>>>>>> 41adbd1 (Update)
        return None

    pv = df.pivot_table(index="fold", columns="model", values="val_RMSE", aggfunc="mean").sort_index()

    plt.figure()
    for col in pv.columns:
        plt.plot(pv.index.to_numpy(), pv[col].to_numpy(), marker="o", label=str(col))
<<<<<<< HEAD

    # This paragraph is for readability: one unstable model can explode and hide the rest,
    # so I default to a log y-axis when RMSE values are strictly positive.
    ymin = np.nanmin(pv.to_numpy())
    if np.isfinite(ymin) and ymin > 0:
        plt.yscale("log")
        plt.ylabel("Validation RMSE (log scale)")
    else:
        plt.yscale("symlog", linthresh=1e-3)
        plt.ylabel("Validation RMSE (symlog)")

    plt.xlabel("Fold")
    plt.title(f"TimeSeriesSplit CV stability (tag={tag})")
    plt.legend()
    plt.grid(True, which="both", axis="y", alpha=0.3)
=======
    plt.xlabel("Fold")
    plt.ylabel("Validation RMSE")
    plt.title(f"TimeSeriesSplit CV stability (tag={tag})")
    plt.legend()
>>>>>>> 41adbd1 (Update)

    out = FIG_DIR / f"cv_stability_{tag}.png"
    _save_fig(out)
    return out


def plot_eda_master_tables() -> Dict[str, Path]:
<<<<<<< HEAD
    # This paragraph is for lightweight EDA figures that describe the dataset in the report,
    # without needing notebooks (GDP dynamics + disaster aggregates + damage distribution).
    df_model, X, y = make_dataset(include_oil=False, include_macro=False, include_covid=False, start_year=None, mode="forecast")
    df = df_model.copy().sort_values("year")

    out_paths: Dict[str, Path] = {}

    # Target series
    if "gdp_growth" in df.columns:
        plt.figure()
        plt.plot(df["year"], df["gdp_growth"])
        plt.axhline(0.0, linewidth=1.0)
        plt.xlabel("Year")
        plt.ylabel("GDP growth (%)")
        plt.title("Japan GDP growth over time")
=======
    # This paragraph is for lightweight EDA figures that describe the dataset in the report
    # (GDP dynamics + disaster aggregates), without needing notebooks.
    df = build_master_table().copy()
    df = df.sort_values("year")
    df["year"] = pd.to_numeric(df["year"], errors="coerce")
    for c in ["gdp_growth", "n_events", "total_damage", "total_deaths", "avg_magnitude", "gdp"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")

    out_paths: Dict[str, Path] = {}

    # This paragraph is for giving readers the target series shape (trend + volatility),
    # because it helps interpret why forecasting is hard.
    if "gdp_growth" in df.columns:
        plt.figure()
        plt.plot(df["year"], df["gdp_growth"])
        plt.axhline(0.0, linewidth=1)
        plt.xlabel("Year")
        plt.ylabel("GDP growth (annual %)")
        plt.title("Japan GDP growth (annual %)")
>>>>>>> 41adbd1 (Update)
        p1 = FIG_DIR / "eda_gdp_growth.png"
        _save_fig(p1)
        out_paths["eda_gdp_growth"] = p1

<<<<<<< HEAD
    # Disaster aggregates
    if "n_events" in df.columns and "total_damage" in df.columns:
        plt.figure()
        plt.plot(df["year"], df["n_events"], label="n_events")
        plt.plot(df["year"], np.log1p(pd.to_numeric(df["total_damage"], errors="coerce").fillna(0.0)), label="log(1+total_damage)")
=======
    # This paragraph is for showing how the disaster signal looks over time,
    # and whether it has enough variation to be useful for prediction.
    if "n_events" in df.columns and "total_damage" in df.columns:
        log_damage = np.log1p(df["total_damage"].fillna(0.0).to_numpy())
        plt.figure()
        plt.plot(df["year"], df["n_events"], label="n_events")
        plt.plot(df["year"], log_damage, label="log(1+total_damage)")
>>>>>>> 41adbd1 (Update)
        plt.xlabel("Year")
        plt.title("Disaster aggregates over time (Japan)")
        plt.legend()
        p2 = FIG_DIR / "eda_disasters.png"
        _save_fig(p2)
        out_paths["eda_disasters"] = p2

<<<<<<< HEAD
        dmg = pd.to_numeric(df["total_damage"], errors="coerce").dropna()
        if len(dmg) > 5:
            # Raw-scale histogram
            plt.figure()
            plt.hist(dmg.clip(lower=0).to_numpy(), bins=30)
=======
        # This paragraph is for scale awareness: damages are heavy-tailed,
        # so a histogram helps explain why I log-transform some features.
        dmg = df["total_damage"].dropna()
        if len(dmg) > 5:
            plt.figure()
            plt.hist(dmg.to_numpy(), bins=30)
>>>>>>> 41adbd1 (Update)
            plt.xlabel("total_damage (USD)")
            plt.title("Damage distribution (raw scale)")
            p3 = FIG_DIR / "eda_damage_dist.png"
            _save_fig(p3)
            out_paths["eda_damage_dist"] = p3

<<<<<<< HEAD
            # Log-scale histogram
            dmg_log = np.log1p(dmg.clip(lower=0))
            plt.figure()
            plt.hist(dmg_log.to_numpy(), bins=30)
            plt.xlabel("log(1 + total_damage)")
            plt.title("Damage distribution (log1p scale)")
            p4 = FIG_DIR / "eda_damage_dist_log.png"
            _save_fig(p4)
            out_paths["eda_damage_dist_log"] = p4

=======
>>>>>>> 41adbd1 (Update)
    return out_paths


# ----------------------------
# Main
# ----------------------------
def main() -> None:
<<<<<<< HEAD
    # This paragraph is for a simple CLI so I can regenerate all report figures in one command.
=======
    # This paragraph is for a simple CLI so I can regenerate all report figures in one command,
    # and so graders can do the same without opening notebooks.
>>>>>>> 41adbd1 (Update)
    parser = argparse.ArgumentParser()
    parser.add_argument("--tag", type=str, default=None, help="Run tag, e.g. run_forecast_strict_main")
    parser.add_argument("--model", type=str, default=None, help="Force model name (optional)")
    parser.add_argument("--top_k", type=int, default=12, help="Top K features for permutation importance")
    args = parser.parse_args()

<<<<<<< HEAD
    # If tag is not provided, I default to the latest metrics file.
=======
    # This paragraph is for convenience: if I forget the tag, I default to the latest metrics file.
>>>>>>> 41adbd1 (Update)
    if args.tag is None:
        latest = _pick_latest_metrics_file()
        tag = latest.stem.replace("model_metrics_", "")
    else:
        tag = args.tag

    metrics = _read_metrics(tag)
<<<<<<< HEAD
=======

    # This paragraph is for consistency: I pick the best-performing model from the saved metrics,
    # unless I explicitly override it from the command line.
>>>>>>> 41adbd1 (Update)
    best_model = _best_model_from_metrics(metrics)
    chosen_model = args.model or best_model

    print(f"[make_report_figures] tag={tag}")
    print(f"[make_report_figures] best_model_from_metrics={best_model} | chosen_model={chosen_model}")
    print(f"[make_report_figures] writing PNG to: {FIG_DIR}")

<<<<<<< HEAD
=======
    # This paragraph is for the report intro/EDA section.
>>>>>>> 41adbd1 (Update)
    eda_out = plot_eda_master_tables()
    if eda_out:
        print("[make_report_figures] EDA figures:", ", ".join(str(p.name) for p in eda_out.values()))

<<<<<<< HEAD
    p_comp = plot_model_comparison(metrics, tag)
    print("[make_report_figures] wrote:", p_comp.name)

    df_model, X_train, X_test, y_train, y_test, years_test, pipe, cfg, used_model = build_predictions_for_tag(
        tag, preferred_model=chosen_model
    )
    pred_df = pd.read_csv(FIG_DIR / f"predictions_{tag}_{used_model}.csv")

    p_pred = plot_actual_vs_pred(pred_df, tag, used_model)
    print("[make_report_figures] wrote:", p_pred.name)

    p_abs = plot_abs_error_by_year(pred_df, tag)
    print("[make_report_figures] wrote:", p_abs.name)

    p_err = plot_errors_vs_disaster_proxy(pred_df, df_model, tag)
    if p_err is not None:
        print("[make_report_figures] wrote:", p_err.name)

    p_imp = plot_permutation_importance(pipe, X_test, y_test, tag, top_k=args.top_k)
    print("[make_report_figures] wrote:", p_imp.name)
=======
    # This paragraph is for the evaluation section: it summarizes test RMSE across models.
    p_comp = plot_model_comparison(metrics, tag)
    print("[make_report_figures] wrote:", p_comp.name)

    # This paragraph is for prediction figures and interpretability figures.
    df_model, X_train, X_test, y_train, y_test, years_test, pipe, cfg = build_predictions_for_tag(
        tag, preferred_model=chosen_model
    )
    pred_df = pd.read_csv(FIG_DIR / f"predictions_{tag}_{chosen_model}.csv")

    p_pred = plot_actual_vs_pred(pred_df, tag, chosen_model)
    print("[make_report_figures] wrote:", p_pred.name)

    p_err = plot_abs_error_by_year(pred_df, tag)
    print("[make_report_figures] wrote:", p_err.name)

    p_scatter = plot_errors_vs_disaster_proxy(pred_df, X_test, tag)
    if p_scatter is not None:
        print("[make_report_figures] wrote:", p_scatter.name)

    p_imp = plot_feature_importance(pipe, X_test, y_test, tag, top_k=int(args.top_k))
    print("[make_report_figures] wrote:", p_imp.name)
    print("[make_report_figures] wrote: feature_importance_table_" + tag + ".csv")
>>>>>>> 41adbd1 (Update)

    p_cv = plot_cv_stability(tag)
    if p_cv is not None:
        print("[make_report_figures] wrote:", p_cv.name)

<<<<<<< HEAD
=======
    print("[make_report_figures] done.")

>>>>>>> 41adbd1 (Update)

if __name__ == "__main__":
    main()
