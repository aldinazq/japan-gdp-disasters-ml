% I provide a fully coherent Overleaf LaTeX report that matches my repository tags, result CSVs, and figure filenames, with smaller figures to fit the page limit.
\documentclass[11pt]{article}

% =========================
% Page format
% =========================
\usepackage[margin=1in]{geometry}

% =========================
% Typography and layout
% =========================
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{setspace}
\setstretch{1.06}

% Control float spacing
\setlength{\textfloatsep}{10pt plus 2pt minus 2pt}
\setlength{\floatsep}{8pt plus 2pt minus 2pt}
\setlength{\intextsep}{8pt plus 2pt minus 2pt}

% =========================
% Math and symbols
% =========================
\usepackage{amsmath, amssymb}

% =========================
% Figures, tables, and floats
% =========================
\usepackage{graphicx}
\graphicspath{{figuresml/}} % Overleaf folder containing the PNGs/CSVs copied from repo figures/
\usepackage{caption}
\captionsetup{font=small,labelfont=bf}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{siunitx}
\sisetup{detect-all=true}
\usepackage{float}
\usepackage{placeins}
\usepackage{needspace}
\usepackage{array}
\usepackage{makecell}
\newcolumntype{L}{>{\raggedright\arraybackslash}X}

% =========================
% Lists and links
% =========================
\usepackage{enumitem}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue}

% =========================
% Citations
% =========================
\usepackage[numbers]{natbib}

% =========================
% Helper macros
% =========================
\newcommand{\code}[1]{\texttt{#1}}

% =========================
% Centered smaller abstract
% =========================
\newenvironment{centeredabstract}{%
  \begin{center}
    \bfseries Abstract
  \end{center}
  \begin{center}
    \begin{minipage}{0.88\linewidth}
      \small
      \noindent
}{%
    \end{minipage}
  \end{center}
  \vspace{1em}
}

% =========================
% FIGURES: smaller size to save pages
% =========================
% Figure stack helper: (filename, caption, label, commentary)
\newcommand{\figstack}[4]{%
\begin{figure}[!htbp]
\centering
% Smaller than before (was width=0.86, height=0.28)
\includegraphics[width=0.78\linewidth,height=0.22\textheight,keepaspectratio]{#1}
\caption{#2}
\label{fig:#3}
{\footnotesize\noindent\emph{Commentary.} #4\par}
\end{figure}
\vspace{0.0em}
}

% A boxed pipeline diagram without extra packages (TikZ-free, robust on Overleaf)
\newcommand{\pipelinebox}[1]{%
\begin{figure}[H]
\centering
\setlength{\fboxsep}{10pt}
\fbox{\begin{minipage}{0.93\linewidth}
\small
#1
\end{minipage}}
\caption{End-to-end pipeline and dataflow used in the project.}
\label{fig:pipeline}
\end{figure}
\vspace{-0.6em}
}

% =========================
% Title
% =========================
\title{Japan GDP Growth Forecasting with Natural Disasters}
\author{Aldina Zeqiri}
\date{}

\begin{document}
\maketitle

% ==========================================================
% Abstract (~200 words)
% ==========================================================
\begin{centeredabstract}
This project builds a reproducible machine learning pipeline to forecast Japan’s annual GDP growth using country-year aggregates of natural disasters. The objective is predictive rather than causal: in the \textbf{strict forecast} setup, predicting GDP growth in year $t$ uses only information observable by the end of year $t-1$. GDP and GDP growth are sourced from the World Bank World Development Indicators (WDI), while disaster outcomes are aggregated from EM-DAT (event counts, deaths, damages, magnitude). To prevent leakage, all preprocessing is executed inside scikit-learn pipelines, rolling features are computed on shifted series, and time order is preserved in both the chronological holdout split and TimeSeriesSplit cross-validation.

Across models (Ridge, Random Forest, Histogram Gradient Boosting, and an MLP benchmark) and strong baselines, the best strict-forecast performance in the long-sample \textbf{main} specification is achieved by Gradient Boosting with test RMSE $\approx 1.50$ (vs.\ a rolling-mean baseline $\approx 2.02$). Restricted-window variants (start year 1992) can have near-zero or negative $R^2$, consistent with small samples and noisy annual aggregates. The project also reports \textbf{nowcast} diagnostics (an upper bound that may use same-year information and, in \texttt{ex\_post} mode, a COVID dummy). In restricted nowcasts, adding oil-related controls substantially improves performance (e.g., Ridge test RMSE $\approx 1.59$, $R^2 \approx 0.40$). Figures explain these patterns via heavy-tailed damages, cross-validation instability for unregularized OLS, and large errors driven by rare structural breaks such as 2020.
\end{centeredabstract}

% ==========================================================
% Introduction (1–1.5 pages)
% ==========================================================
\section{Introduction}
Natural disasters can affect macroeconomic outcomes through capital destruction, production interruptions, supply-chain propagation, and reconstruction dynamics. The empirical macro literature emphasizes that impacts are heterogeneous across countries, event types, and institutional environments, and that aggregate effects are difficult to estimate precisely \citep{noy2009,cavallo_noy2011,felbermayr2014}. Japan provides salient examples where large shocks plausibly transmit through networks and supply chains \citep{carvalho2021}. At the same time, climate-related extremes are an increasingly important source of macro risk in the long run \citep{ipcc2021}, which motivates testing whether disaster databases contain usable predictive signals for macro forecasting.

This project frames the problem as a forecasting benchmark. The target is Japan’s annual GDP growth in year $t$, with a one-year-ahead horizon. A strict timing rule is enforced: predictors for year $t$ must be observable at $t-1$. This matters because many disaster outcomes (especially damages) are finalized during or after year $t$, and using contemporaneous measures can overstate real-time predictability.

The contribution is practical and methodological. The repository implements a reproducible pipeline, compares multiple models to strong baselines under time-series validation, and reports diagnostics that explain success and failure cases. The goal is not to claim “ML wins,” but to show when models beat persistence baselines, when they collapse to mean-like predictions, and how small annual samples and structural breaks limit what is achievable.

% ==========================================================
% Background (1–1.5 pages)
% ==========================================================
\section{Background}
A large empirical literature studies disasters and growth. Survey evidence highlights that average effects vary widely and depend on exposure, preparedness, and economic structure \citep{cavallo_noy2011,felbermayr2014}. Some work finds persistent output losses after catastrophic events, especially when damages are large relative to economic size. For Japan, supply-chain propagation can amplify localized shocks \citep{carvalho2021}. These mechanisms suggest that disaster information \emph{could} matter for forecasting, but they also imply that coarse annual aggregates might miss timing and sectoral transmission.

Forecasting with annual macro data is intrinsically difficult: sample sizes are small and rare crises dominate losses. Standard guidance in time-series forecasting stresses leakage-free evaluation, strong baselines, and careful split design \citep{hyndman2021,bergmeir2018}. This motivates the project’s architecture: (i) a strict ex-ante forecasting mode, (ii) transparent baselines (mean and rolling means), and (iii) both a chronological holdout and TimeSeriesSplit cross-validation for stability checks.

% ==========================================================
% Design & Architecture (2 pages)
% ==========================================================
\section{Design \& Architecture}

\subsection{Repository structure (reproducibility-first)}
The codebase separates raw inputs, feature construction, modeling, and artifacts:
\begin{itemize}[leftmargin=*]
  \item \texttt{src/data\_loading.py} loads and standardizes spreadsheet inputs from \texttt{data/}.
  \item \texttt{src/features.py} builds the cleaned yearly master table by aggregating EM-DAT to year level and merging with WDI GDP/growth (plus optional macro/oil series).
  \item \texttt{src/models.py} constructs lag/rolling features, applies strict-vs-nowcast timing rules, trains baselines and ML models, runs time-series validation, and writes outputs to \texttt{results/}.
  \item \texttt{main.py} is the CLI entry point that runs benchmarks and produces the saved \texttt{results/} artifacts used by plots and dashboards.
  \item \texttt{tests/} contains unit/integration tests that enforce correctness and reproducibility (e.g., no shuffled years).
  \item Optional scripts generate artifacts: \texttt{scripts/make\_report\_figures.py} exports report-ready PNGs/CSVs, and \texttt{dashboard/build\_dashboard.py} builds an HTML dashboard.
\end{itemize}

\subsection{Pipeline dataflow and run modes (forecast vs nowcast; strict vs ex\_post)}
\noindent\textbf{Forecast vs nowcast.}
Forecast mode enforces the strictest timing rule: to predict $gdp\_growth_t$, predictors must be known by $t-1$ (disaster and control variables are used as lagged features). Nowcast mode is a diagnostic upper bound: it may use same-year information (when present in the feature set) to approximate what could be achieved with contemporaneous data.

\noindent\textbf{Strict vs ex\_post.}
In code, \texttt{--covid-mode strict} excludes an ex-post indicator. \texttt{--covid-mode ex\_post} allows a simple COVID dummy (year $\geq$ 2020) as a robustness control that is not a genuine “forecastable” variable in real time. Ex\_post results are therefore interpreted as a robustness/diagnostic variant, not the headline strict forecasting claim.

\pipelinebox{
\textbf{Inputs} (\code{data/}) \\
\quad $\rightarrow$ WDI GDP \& GDP growth (Japan, annual) \\
\quad $\rightarrow$ EM-DAT events (Japan, event-level) \\
\quad $\rightarrow$ optional macro controls + oil series (annual) \\[0.3em]
\textbf{Master table} (\code{src/features.py}) \\
\quad $\rightarrow$ aggregate EM-DAT to country-year: counts, deaths, total damage, average magnitude \\
\quad $\rightarrow$ merge with WDI by year; align calendar years \\
\quad $\rightarrow$ fill disaster aggregates with zero when no event is recorded that year \\[0.3em]
\textbf{Feature engineering} (\code{src/models.py}) \\
\quad $\rightarrow$ GDP dynamics: lags + rolling mean/std (computed from past values only) \\
\quad $\rightarrow$ transformations: log(1+damage), damage share of GDP, time trend (\(year, year^2\)) \\
\quad $\rightarrow$ strict forecast: use lagged versions of disaster/macro/oil features (t-1) \\
\quad $\rightarrow$ nowcast: optionally include same-year covariates if present \\[0.3em]
\textbf{Modeling \& validation} (\code{src/models.py}) \\
\quad $\rightarrow$ chronological holdout test window (last fraction of years) \\
\quad $\rightarrow$ time-series CV via TimeSeriesSplit (stability check) \\
\quad $\rightarrow$ compare baselines + ML models; save metrics (and CV scores) to \code{results/} \\[0.3em]
\textbf{Artifacts (repo)} \\
\quad $\rightarrow$ \code{results/model\_metrics\_<tag>.csv} \\
\quad $\rightarrow$ \code{results/cv\_scores\_<tag>.csv} \\
\quad $\rightarrow$ \code{figures/predictions\_<tag>\_<best\_model>.csv} \\
\quad $\rightarrow$ \code{figures/*.png} (then copied into Overleaf \code{figuresml/}) \\
\quad $\rightarrow$ optional HTML dashboards in \code{dashboard/}
}

\subsection{Key design choices}
First, the strict ex-ante rule is treated as a contract: to forecast year $t$, the pipeline only uses predictors available by $t-1$. This is implemented directly in \code{src/models.py} by constructing lagged versions of disaster and control variables and by computing rolling GDP statistics from shifted series.

Second, baselines are treated as serious competitors rather than placeholders. With small annual samples, simple persistence rules can be hard to beat. The project therefore includes mean and rolling-mean baselines so that any ML “gain” is interpreted relative to realistic alternatives rather than relative to a weak benchmark.

Third, multiple feature sets are used to separate signals. The “main” specification uses a long sample and includes GDP persistence plus lagged disaster aggregates. “Restricted” variants start in 1992 to test stability in more recent decades and to align with shorter-availability controls. Optional macro and oil variants test whether broad macro shocks add predictive value beyond disasters.

Fourth, model complexity is balanced against overfitting risk. Ridge regression provides a regularized linear benchmark \citep{ridge}. Random Forest and Histogram Gradient Boosting allow nonlinearities \citep{rf,gbm}. An MLP is included as a high-capacity stress test; when it fits training extremely well but generalizes poorly, that is used as evidence of variance/overfitting rather than as a success.

\subsection{Leakage prevention and validation protocol}
Leakage prevention is enforced mechanically. Rolling features are computed from shifted GDP growth so that the rolling window ends at $t-1$. Strict-mode covariates are shifted by one year. Preprocessing (imputation and scaling where relevant) is done inside scikit-learn pipelines so that each fold fits transformations on the training subset only.

Validation follows time-series best practice. A chronological holdout measures final out-of-sample performance. TimeSeriesSplit cross-validation provides a stability check across multiple historical folds \citep{bergmeir2018}. Early folds are intentionally difficult because training windows are small; this explains why some models show unstable fold RMSE even when the final holdout comparison is stable.

\subsection{Dataset coverage and evaluation windows (exact years)}
To make results fully auditable, Table~\ref{tab:windows} reports the exact years and sample sizes used by the configurations cited in the Evaluation section. The holdout split is deterministic with \code{test\_ratio = 0.2}.

\begin{table}[H]
\centering
\small
\begin{tabularx}{\linewidth}{l l X}
\toprule
Run cited in report & Tag (CSV suffix) & Years used (N), Train window, Test window \\
\midrule
Strict forecast (main, tuned RF run) &
\code{run\_forecast\_strict\_tunerf\_main} &
1962--2023 (62); Train: 1962--2010 (49); Test: 2011--2023 (13) \\
Nowcast (ex\_post, restricted) &
\code{run\_nowcast\_ex\_post\_restricted} &
1992--2023 (32); Train: 1992--2016 (25); Test: 2017--2023 (7) \\
Nowcast (ex\_post, restricted\_oil) &
\code{run\_nowcast\_ex\_post\_restricted\_oil} &
1992--2023 (32); Train: 1992--2016 (25); Test: 2017--2023 (7) \\
\bottomrule
\end{tabularx}
\caption{Exact evaluation windows for the configurations cited in the report.}
\label{tab:windows}
\end{table}

\subsection{Feature-set summary (what changes across runs)}
\begin{table}[H]
\centering
\small
\begin{tabularx}{\linewidth}{l X}
\toprule
Run configuration & Included feature groups (timing enforced by the run) \\
\midrule
\textbf{Main} & Time trend + lagged GDP persistence (lags, rolling mean/std) + lagged disaster aggregates (counts, deaths, damages, magnitude, share) \\
\textbf{Restricted} & Same construction rules as Main, but evaluated on years $\geq$ 1992 to test stability on a later subsample \\
\textbf{Restricted\_macro} & Restricted window + lagged macro controls (when available) \\
\textbf{Restricted\_oil} & Restricted window + oil-related features (and optionally their changes) \\
\bottomrule
\end{tabularx}
\caption{Conceptual summary of run configurations used by \code{main.py}.}
\label{tab:featuresets}
\end{table}

\subsection{Artifacts and outputs}
The project produces machine-readable artifacts that are used consistently by the report, dashboards, and tests.

\paragraph{Run outputs (written by \texttt{main.py} / \texttt{models.py} into \texttt{results/}).}
\begin{itemize}[leftmargin=*]
  \item \texttt{results/model\_metrics\_<tag>.csv}: train/test RMSE, MAE, \(R^2\), and diagnostic counts (+ run metadata such as \code{start\_year}, \code{test\_ratio}, \code{n\_train}, \code{n\_test}, \code{n\_features}).
  \item \texttt{results/cv\_scores\_<tag>.csv}: fold-level RMSE for time-series CV stability plots.
\end{itemize}

\paragraph{Reporting artifacts (generated by \texttt{scripts/make\_report\_figures.py} into \texttt{figures/}).}
\begin{itemize}[leftmargin=*]
  \item \texttt{figures/predictions\_<tag>\_<best\_model>.csv}: year-by-year predictions for the best model in that run (selected by lowest test RMSE).
  \item \texttt{figures/*.png}: report-ready figures (comparison, CV stability, feature importance, EDA).
\end{itemize}
\noindent For Overleaf, I copy the generated PNGs (and any CSVs needed) from repo \texttt{figures/} into the Overleaf folder \texttt{figuresml/}, which is why the report uses \verb|\graphicspath{{figuresml/}}|.

% ==========================================================
% Implementation (2–3 pages)
% ==========================================================
\section{Implementation}

\subsection{Data sources and preprocessing}
GDP and GDP growth come from WDI \citep{wdi}. Disaster information comes from EM-DAT \citep{emdat}. The project aggregates EM-DAT at the Japan-year level (counts, deaths, total damages, average magnitude), merges those aggregates with WDI by calendar year, and then constructs forecasting features in \code{src/models.py}.

Two implementation details matter for correctness. First, disaster years with no recorded events are treated as zeros in the merged master table. Second, damage values are heavy-tailed, so the model uses \(\log(1+\text{damage})\) and damage share of GDP to stabilize scale and reduce the influence of extreme outliers.

\subsection{Feature engineering (how each feature is computed)}
GDP persistence is represented by $gdp\_growth_{t-1}$ and $gdp\_growth_{t-2}$ plus rolling mean/std features computed from shifted series (so the window ends at $t-1$). Disaster aggregates are transformed (log damage, damage share) and then lagged under strict forecasting. In nowcast mode, the pipeline may additionally use same-year covariates if they are part of the feature set (diagnostic upper bound).

The pipeline also includes a smooth time trend (\(year\), \(year^2\)) because long-run macro drift can otherwise be misattributed to other predictors. Time is always known at forecasting time, so including it does not violate the ex-ante rule.

\subsection{Missing values, scaling, and leakage-proof preprocessing}
Feature availability differs by configuration because some macro/oil series start later. Rather than manually trimming datasets, the pipeline defines a consistent dataset contract: rows must have the target and essential GDP lag feature; remaining missing predictors are handled inside the sklearn pipeline via an imputer. Models that are sensitive to scaling (Ridge, MLP) use \code{StandardScaler} after imputation; tree models are evaluated with the same imputed inputs for comparability.

\subsection{Model training and validation}
All models are evaluated with a chronological holdout split (last fraction of years as test). TimeSeriesSplit cross-validation is used to assess stability across folds. Baselines are implemented as first-class models (mean, last-year, rolling mean) to keep comparisons honest. The benchmark code saves summary metrics and CV scores to \texttt{results/}. Per-year predictions used in plots are exported by \texttt{scripts/make\_report\_figures.py} to \texttt{figures/}, making figures and diagnostics reproducible.

\subsection{Hyperparameter policy}
Hyperparameters are conservative by default because annual time series provide limited observations. Over-tuning can overfit the validation procedure itself, especially when early folds have short training windows. Optional tuning flags exist for selected models; for the strict-forecast report run we enable Random Forest tuning using train-only time-series CV, while keeping other choices conservative (see Appendix~\ref{app:hyperparams}).

\subsection{Testing and contracts (what the tests enforce)}
The test suite is designed to catch silent mistakes that would invalidate the forecasting story (e.g., shuffled years, missing required columns, inconsistent output artifacts). Unit tests check that the master table contains required variables and that key transformations behave as intended. Integration tests run end-to-end and verify that outputs are produced deterministically.

% ==========================================================
% Evaluation (1–2 pages)
% ==========================================================
\section{Evaluation}

\subsection{Headline results with baselines}
Table~\ref{tab:headline} reports two representative settings using the values produced by saved \texttt{model\_metrics} outputs:
\texttt{results/model\_metrics\_run\_forecast\_strict\_tunerf\_main.csv} and
\texttt{results/model\_metrics\_run\_nowcast\_ex\_post\_restricted\_oil.csv}.

\begin{table}[H]
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
\begin{tabularx}{\linewidth}{L L S[table-format=1.3] S[table-format=1.3] S[table-format=1.3] L S[table-format=1.3]}
\toprule
Setting & Best model & {RMSE} & {$R^2$} & {MAE} & Best baseline & {\makecell{Baseline\\RMSE}} \\
\midrule
Forecast (strict, main) & Gradient Boosting & 1.500 & 0.131 & 1.039 & Roll-3 mean & 2.016 \\
Nowcast (ex\_post, restricted\_oil) & Ridge & 1.592 & 0.401 & 1.416 & Mean (train) & 2.102 \\
\bottomrule
\end{tabularx}
\caption{Headline test-set results with baseline comparison (values from saved \texttt{results/model\_metrics\_*.csv}).}
\label{tab:headline}
\end{table}

On the long-sample main specification, Gradient Boosting improves test RMSE by about 25\% relative to the rolling-mean baseline, suggesting incremental predictive content beyond persistence. In restricted nowcast diagnostics, adding oil features yields a large improvement and a substantially higher \(R^2\) on the shorter evaluation window.

\subsection{Restricted nowcast results (with and without oil)}
Table~\ref{tab:restricted_nowcast} focuses on restricted nowcasts and quantifies the improvement from adding oil features using:
\texttt{results/model\_metrics\_run\_nowcast\_ex\_post\_restricted.csv} and
\texttt{results/model\_metrics\_run\_nowcast\_ex\_post\_restricted\_oil.csv}.
“Severe disaster” years are defined as years whose damage-share proxy exceeds the 85th percentile of the training distribution; because the restricted test window is short, the number of severe test years is small.

\begin{table}[H]
\centering
\small
\begin{tabularx}{\linewidth}{l l S[table-format=1.3] S[table-format=1.3] S[table-format=1.3] c}
\toprule
Setting & Model & {RMSE test} & {$R^2$ test} & {RMSE severe} & {$n$ severe} \\
\midrule
Nowcast (ex\_post, restricted) & Ridge & 2.012 & 0.043 & 2.390 & 3 \\
Nowcast (ex\_post, restricted\_oil) & Ridge & 1.592 & 0.401 & 2.043 & 3 \\
\bottomrule
\end{tabularx}
\caption{Restricted nowcast performance for Ridge with and without oil features (values from saved \texttt{results/model\_metrics\_*.csv}).}
\label{tab:restricted_nowcast}
\end{table}

\Needspace{0.45\textheight}
\subsection{Prediction diagnostics and model behavior (strict forecast main)}
In the strict forecast main run, a small number of extreme years dominate the test error. The COVID recession year (2020) is the largest error: the model under-predicts the magnitude of the downturn, which is expected when training on a small annual dataset with few crisis observations.

\FloatBarrier

\figstack{pred_vs_actual_run_forecast_strict_tunerf_main.png}
{Strict forecast (main): Predicted vs actual GDP growth for Gradient Boosting.}
{pred_vs_actual_main}
{The model tracks moderate fluctuations but underestimates rare structural breaks (notably 2020), which drive a large share of test loss.}

\figstack{model_comparison_run_forecast_strict_tunerf_main.png}
{Model comparison (strict forecast, main): test RMSE across baselines and ML models.}
{model_comparison_main}
{Gradient Boosting is best on the holdout test window. Random Forest is competitive. Unregularized linear regression and the MLP generalize poorly in this small-sample setting.}

\figstack{cv_stability_run_forecast_strict_tunerf_main.png}
{Time-series cross-validation stability (strict forecast, main).}
{cv_stability_main}
{Fold RMSE can be volatile because early folds have short training windows. Linear regression shows extreme instability in some folds (note the log-scaled y-axis), while regularized and tree-based models are more stable.}

\figstack{feature_importance_run_forecast_strict_tunerf_main.png}
{Permutation feature importance for the selected model (strict forecast, main).}
{feature_importance_main}
{GDP persistence and volatility proxies (lags and rolling std) dominate predictive content. Disaster aggregates contribute limited incremental signal at annual frequency once GDP dynamics are included.}

\figstack{error_by_year_run_forecast_strict_tunerf_main.png}
{Absolute error by year (strict forecast, main).}
{error_by_year_main}
{A few years account for most loss. The largest bar corresponds to the COVID recession year, consistent with “rare breaks dominate RMSE” in annual macro forecasting.}

\figstack{errors_vs_disaster_run_forecast_strict_tunerf_main.png}
{Errors vs disaster intensity proxy (strict forecast, main).}
{errors_vs_disaster_main}
{There is no strong monotonic relationship between error size and the lagged disaster proxy, which is consistent with limited incremental predictive content from annual disaster aggregates.}

\subsection{Exploratory data analysis (context for the forecasting difficulty)}
EDA provides context for why the task is hard. GDP growth has rare extreme years, and disaster damages are heavy-tailed, which increases sensitivity to a small number of observations and elevates overfitting risk.

\figstack{eda_gdp_growth.png}
{EDA: Japan annual GDP growth over time.}
{eda_gdp_growth}
{The series features a few extreme downturns that dominate forecast error, so stability and baselines matter.}

\figstack{eda_disasters.png}
{EDA: Disaster aggregates over time (annual Japan totals).}
{eda_disasters}
{Annual disaster measures are intermittent and can reflect both true event frequency and reporting/measurement changes, limiting stable signal.}

\figstack{eda_damage_dist_log.png}
{EDA: Distribution of reported damages (log1p scale).}
{eda_damage_dist_log}
{Reported damages are heavy-tailed; the log transform makes the mass of “small-to-medium” events visible and avoids the plot being dominated by a few extreme years.}

% ==========================================================
% Discussion (1 page)
% ==========================================================
\section{Discussion}

\subsection{Interpreting what the models are really learning}
The strict-forecast main results show that the best model mainly leverages GDP persistence and volatility structure (lags and rolling statistics). This is not a weakness: in annual macro data, persistence is often the dominant predictable component. The feature-importance plot supports this directly, with rolling volatility and lagged GDP features contributing far more than disaster aggregates.

This pattern is consistent with the disasters-and-growth literature, which argues that aggregate disaster impacts are heterogeneous and can be hard to summarize in a stable country-year mapping \citep{cavallo_noy2011,felbermayr2014}. Annual aggregates may blur timing (within-year), omit sectoral exposure, and miss policy response, all of which matter for macro outcomes. As a result, disasters can be economically meaningful yet still provide limited \emph{incremental} forecasting power in this particular setup.

\subsection{What the diagnostics reveal (and why they matter)}
Several diagnostics explain the observed performance differences:
\begin{itemize}[leftmargin=*]
  \item \textbf{Linear regression instability in CV.} The CV stability figure shows extreme fold RMSE spikes for unregularized linear regression. With correlated predictors (lags, rolling means, trends) and small samples, OLS can be numerically unstable. Ridge improves stability by shrinking coefficients.
  \item \textbf{MLP overfitting.} The MLP achieves near-zero training RMSE but poor test RMSE, which is classic high-variance behavior in small annual datasets. In this context it is a useful stress test: it confirms that model capacity alone does not create signal.
  \item \textbf{Rare breaks dominate loss.} The year-by-year error plot shows that the largest losses come from rare structural breaks (especially 2020). A model trained on decades of ``normal'' years cannot reliably anticipate unprecedented shocks using only lagged aggregates.
\end{itemize}

\subsection{Why the oil feature helps in restricted nowcasts}
In the restricted nowcast diagnostics, adding oil features materially improves fit. This is coherent with the economic context: Japan is sensitive to global energy price shocks, and oil variables can proxy broad macro conditions that are not captured by disaster aggregates alone. The improvement does not imply disasters are unimportant; it implies that, for annual GDP growth forecasting, broad macro shocks can be more consistently informative than coarse disaster totals.

\subsection{Limitations (stated as constraints, not excuses)}
The main limitations are structural rather than implementation-specific. Annual data imply small samples, especially in restricted windows, which increases variance and makes time-series CV unstable. Disaster measures (in particular reported damages) are noisy and heavy-tailed, and annual aggregation blurs within-year timing and sectoral transmission. Finally, rare structural breaks (e.g., 2020) dominate forecast errors and cannot be learned reliably from a handful of crisis observations. To mitigate methodological risks, the pipeline enforces strict timing (t-1 information only), computes rolling features on shifted series, and fits all preprocessing inside sklearn pipelines to prevent leakage.

% ==========================================================
% Conclusion (0.5 page)
% ==========================================================
\section{Conclusion}
This project delivers a reproducible ML pipeline to forecast Japan’s annual GDP growth using lagged disaster aggregates and optional controls, with leakage-free time-series validation. In the long-sample strict-forecast main configuration, Gradient Boosting improves meaningfully over rolling baselines, indicating incremental predictive content beyond simple persistence rules. However, disaster aggregates add limited incremental signal once GDP dynamics are included, and flexible models can overfit under small-sample constraints.

Restricted nowcast diagnostics show that adding oil-related information improves performance substantially, suggesting that broad macro shocks can be more predictive for annual growth than coarse disaster totals. Future work would benefit from higher-frequency outcomes, richer exposure measures (sectoral or regional), and explicit handling of structural breaks.

% ==========================================================
% References (manual bib; not counted in the 10 pages of main text)
% ==========================================================
\clearpage
\begin{thebibliography}{99}

\bibitem{wdi}
World Bank. \textit{World Development Indicators (WDI)}.

\bibitem{emdat}
CRED / UCLouvain. \textit{EM-DAT: The International Disaster Database}. Dataset used for disaster aggregates.

\bibitem{ipcc2021}
IPCC. \textit{Climate Change 2021: The Physical Science Basis (AR6 WG1)}. Cambridge University Press, 2021.

\bibitem{noy2009}
Ilan Noy. ``The macroeconomic consequences of disasters.'' \textit{Journal of Development Economics}, 88(2), 221--231, 2009.

\bibitem{cavallo_noy2011}
Eduardo Cavallo and Ilan Noy. ``Natural Disasters and the Economy---A Survey.'' \textit{International Review of Environmental and Resource Economics}, 5(1), 63--102, 2011.

\bibitem{felbermayr2014}
Gabriel Felbermayr and Jasmin Gr\"oschl. ``Naturally negative: The growth effects of natural disasters.'' \textit{Journal of Development Economics}, 111, 92--106, 2014.

\bibitem{carvalho2021}
Vasco M. Carvalho, Makoto Nirei, Yukiko U. Saito, and Alireza Tahbaz-Salehi. ``Supply Chain Disruptions: Evidence from the Great East Japan Earthquake.'' \textit{Quarterly Journal of Economics}, 136(2), 1255--1321, 2021.

\bibitem{hyndman2021}
Rob J. Hyndman and George Athanasopoulos. \textit{Forecasting: Principles and Practice} (3rd ed.), 2021.

\bibitem{bergmeir2018}
Christoph Bergmeir, Rob J. Hyndman, and Bonsoo Koo. ``A note on the validity of cross-validation for evaluating autoregressive time series prediction.'' \textit{Computational Statistics \& Data Analysis}, 2018.

\bibitem{rf}
Leo Breiman. ``Random Forests.'' \textit{Machine Learning}, 45, 5--32, 2001.

\bibitem{gbm}
Jerome H. Friedman. ``Greedy function approximation: A gradient boosting machine.'' \textit{Annals of Statistics}, 29(5), 1189--1232, 2001.

\bibitem{ridge}
Arthur E. Hoerl and Robert W. Kennard. ``Ridge Regression: Biased Estimation for Nonorthogonal Problems.'' \textit{Technometrics}, 12(1), 55--67, 1970.

\end{thebibliography}

% ==========================================================
% Appendices (optional; not counted in 10 pages)
% ==========================================================
\clearpage
\section*{Appendix (supplementary material)}
The remainder of this document provides supplementary material (reproducibility notes, hyperparameter details, AI tool disclosure, and extra figures).
\appendix

\section{Extra EDA figure (raw damage scale)}
\figstack{eda_damage_dist.png}
{EDA: Distribution of reported damages (raw scale).}
{eda_damage_dist_raw}
{On the raw USD scale, a small number of extreme observations dominate the histogram. This is why log transforms are used for modeling and for readable EDA plots.}

\section{Hyperparameters and tuning grids (as implemented)}
\label{app:hyperparams}
\noindent This appendix summarizes the defaults and the optional tuning grids implemented in \texttt{src/models.py}. These are included for transparency; the headline results use conservative choices to reduce overfitting risk in small annual samples.

\begin{table}[H]
\centering
\small
\begin{tabularx}{\linewidth}{l X}
\toprule
Model & Defaults / tuning grid used in code \\
\midrule
Ridge &
Default pipeline uses \code{Ridge(alpha=5.0, random\_state=42)} with imputation + standardization. \\
Random Forest &
Default pipeline uses \code{n\_estimators=600}, \code{max\_depth=5}, \code{min\_samples\_leaf=2}.
If \code{--tune-rf}: grid over \code{n\_estimators \{200,400,600\}}, \code{max\_depth \{None,5,10\}}, \code{min\_samples\_split \{2,5\}}, \code{min\_samples\_leaf \{1,2\}} with TimeSeriesSplit CV. \\
Histogram Gradient Boosting &
Default uses \code{HistGradientBoostingRegressor(learning\_rate=0.05, max\_depth=3, max\_iter=600, l2\_regularization=0.1, early\_stopping=False, random\_state=42)}.
If \code{--tune-gb}: grid over
\code{learning\_rate \{0.03,0.05,0.1\}}, \code{max\_depth \{2,3,4\}}, \code{max\_iter \{300,600,1200\}},
\code{l2\_regularization \{0.0,0.1,1.0\}}. \\
MLP &
Benchmark uses \code{hidden\_layer\_sizes=(32,16)}, \code{alpha=1e-3}, \code{max\_iter=5000}, with \code{early\_stopping=False}, plus imputation + standardization. \\
\bottomrule
\end{tabularx}
\caption{Hyperparameter policy matching the implementation in \texttt{src/models.py}.}
\end{table}

\section{Reproducibility (optional)}
\textbf{(A) Strict forecast main run used for the report figures (tag: \texttt{run\_forecast\_strict\_tunerf\_main}).}
\begin{verbatim}
python3 main.py --mode forecast --covid-mode strict --only-main --tune-rf
\end{verbatim}

\textbf{(B) Full strict forecast suite (all forecast configurations).}
\begin{verbatim}
python3 main.py --mode forecast --covid-mode strict --tune-rf
\end{verbatim}

\textbf{(C) Nowcast ex\_post suite (needed for the nowcast tables).}
\begin{verbatim}
python3 main.py --mode nowcast --covid-mode ex_post
\end{verbatim}

\textbf{Build report figures (copy PNGs into Overleaf folder figuresml/).}
\begin{verbatim}
python3 scripts/make_report_figures.py --tag run_forecast_strict_tunerf_main
\end{verbatim}

\textbf{Build dashboard (optional).}
\begin{verbatim}
python3 dashboard/build_dashboard.py --tag run_forecast_strict_tunerf_main
\end{verbatim}

\textbf{Key outputs.}
\begin{itemize}[leftmargin=*]
  \item \texttt{results/model\_metrics\_*.csv}: summary metrics per run.
  \item \texttt{results/cv\_scores\_*.csv}: CV fold RMSE per run.
  \item \texttt{figures/predictions\_*.csv}: year-by-year test predictions exported by the reporting script.
  \item \texttt{figures/*.png}: report-ready figures exported by the reporting script.
  \item \texttt{dashboard/*.html}: interactive dashboards.
\end{itemize}

\section{AI tool usage disclosure (optional)}
I used AI tools as assistance \textbf{as needed} during development:
\begin{itemize}[leftmargin=*]
  \item \textbf{ChatGPT}: debugging help, pipeline improvements (time-series validation, artifacts), occasional small code snippets, and English writing edits (clarity, structure).
  \item \textbf{GitHub Copilot}: auto-completion and small syntax adjustments.
\end{itemize}
I take full responsibility for the final code, results, and interpretation.

\end{document}
