# Japan GDP Growth Forecasting with Natural Disasters (Machine Learning Project)

This repository builds a **reproducible machine learning pipeline** to **forecast Japan’s annual GDP growth** using **lagged disaster aggregates** (EM-DAT) and macro controls. The project emphasizes **time-series validation** best practices (no random shuffling). **Final report:** `project_report.pdf` (LaTeX sources in `report/`).


---

## Prediction Task

- **Target:** `gdp_growth` in year **t** (annual %, Japan)
- **Horizon:** one-year-ahead forecasting
- **Strict ex-ante setup:** to predict `gdp_growth_t`, the model uses only information observable at **t−1**:
  - lagged GDP dynamics: `gdp_growth_lag1`, `gdp_growth_lag2`, rolling mean/std based on past values
  - lagged disaster aggregates: number of events, deaths, damage, magnitude (all at t−1)
  - optional lagged macro controls (WDI): inflation, unemployment, exports/GDP, investment/GDP, FX (all at t−1)
  - optional lagged oil controls: oil price and oil price change (all at t−1)

---

## Repository Structure

```text
.
├── AI_USAGE.md
├── PROPOSAL.md
├── README.md
├── project_report.pdf          
├── report/                  
│   ├── main.tex
│   └── figuresml/                
├── main.py
├── requirements.txt
├── data/
├── figures/                    
├── results/
├── scripts/
│   └── make_report_figures.py
├── dashboard/
│   └── build_dashboard.py
├── src/
│   ├── __init__.py
│   ├── data_loading.py
│   ├── features.py
│   └── models.py
└── tests/
    ├── test_features.py
    ├── test_pipeline.py
    └── test_models.py
```

---

## Key modules

- `src/data_loading.py`: loads the input Excel files from `data/` (WDI GDP/GDP growth, EM-DAT events, optional macro indicators, optional oil series)
- `src/features.py`: builds the yearly Japan master table and disaster aggregates; computes derived variables (e.g., `log1p(total_damage)`, `damage_share_gdp`)
- `src/models.py`: constructs feature sets, trains models, runs time-series CV, and writes run outputs to `results/`
- `main.py`: CLI entry point to run benchmarks (non-interactive)
- `dashboard/build_dashboard.py`: optional HTML dashboard generator (diagnostics)
- `scripts/make_report_figures.py: generates report figures and exports per-year predictions used in plots to figures/ (then copied into report/figuresml/ for LaTeX/Overleaf)`
- `tests/`: unit tests for data integrity and pipeline reproducibility


---

## Data

Input files are stored in `data/` (Excel):

- World Bank / WDI GDP and GDP growth (Japan)
- EM-DAT disasters for Japan (event-level; aggregated to yearly)
- Optional WDI macro indicators (e.g., inflation, unemployment, exports/GDP, investment/GDP, FX)
- Optional oil price series

**Unit note:** EM-DAT damages are commonly reported in *thousands of USD* (often shown as “'000 US$”). This project standardizes damage units consistently in preprocessing (see `src/features.py`) and uses `log1p(total_damage)` / `damage_share_gdp` to stabilize scale.



---



## Quickstart 

If you want the shortest, safest run (recommended for graders):

```
python3 -m venv .venv
source .venv/bin/activate
python3 -m pip install -r requirements.txt
python3 -m pytest -q
python3 main.py --only-main --tune-rf 
```

---

## Setup

Create and activate a virtual environment, then install dependencies:

1) Create venv  
`python3 -m venv .venv`

2) Activate  
`source .venv/bin/activate`

3) Install  
`python3 -m pip install --upgrade pip`  
`python3 -m pip install -r requirements.txt`

---

## Run the Project

- Default run:  
  `python3 main.py`

- Main benchmark: strict one-year-ahead forecast:  
  `python3 main.py --mode forecast --covid-mode strict`

- Ex-post mode (for explanation/robustness only):  
  `python3 main.py --mode forecast --covid-mode ex_post`

- Nowcast mode (optional):  
  `python3 main.py --mode nowcast --covid-mode strict`

- Nowcast ex-post (optional):  
  `python3 main.py --mode nowcast --covid-mode ex_post`

- Only run the main block (skip additional robustness runs):  
  `python3 main.py --only-main`
  
- Hyperparameter tuning :  
  `python3 main.py --tune-rf`  
  `python3 main.py --tune-gb`

- Change the test split ratio:  
  `python3 main.py --test-ratio 0.25`


---

## Outputs

The project produces machine-readable artifacts that are used consistently by the report, dashboards, and tests.

**Run outputs (written by `main.py` / `models.py` into `results/`):**
- `results/model_metrics_<tag>.csv`: train/test RMSE, MAE, R2, and diagnostic counts (+ run metadata).
- `results/cv_scores_<tag>.csv`: fold-level RMSE for time-series CV stability plots.
- `results/best_params_<tag>_<model>.csv` (optional): written only when tuning is enabled (selected hyperparameters).

**Reporting artifacts (generated by `scripts/make_report_figures.py` into `figures/`):**
- `figures/predictions_<tag>_<model>.csv`: year-by-year predictions used in plots (exported by the reporting script, not by `main.py` directly).
- `figures/*.png`: plots used in the LaTeX report (generated by the reporting script).
  

### Post-disaster diagnostics (added value)

In addition to overall test performance, the project reports performance for:

- **post-disaster years:** years where `n_events_lag1 > 0`
- **severe disasters:** defined using a train-only quantile threshold (**q = 0.85**) on `damage_share_gdp_lag1` (no test peeking). The threshold is computed on positive train values when enough positives exist.


---

## Dashboard (Optional)

Generate the HTML dashboard (reads saved outputs from `results/`):

`python3 dashboard/build_dashboard.py`

Open all dashboards:

`open dashboard/*.html`

(Option) Build a dashboard for a specific run tag:

`python3 dashboard/build_dashboard.py --tag run_forecast_strict_tunerf_main`


---

## Report Figures (PNG for LaTeX)

Generate PNG figures for the report (saved to `figures/`):

`python3 scripts/make_report_figures.py --tag run_forecast_strict_tunerf_main`

Then check:

`ls figures`


---

## Run Tests

`python3 -m pytest -q`


---

## Reproducibility Notes

- No shuffling is used for evaluation (time-based splits only).
- Randomized models use a fixed seed when applicable (`random_state=42`).
- Missing values are handled through imputation inside model pipelines.

---

## AI Tool Usage

AI usage disclosure is provided in `AI_USAGE.md`.
